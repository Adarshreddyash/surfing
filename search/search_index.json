{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udf0a Surfing Weights","text":"<p>Welcome to Surfing Weights - a Python server for streaming transformer model weights to enable efficient AI inference on edge devices, IoT, and mobile platforms.</p>"},{"location":"#overview","title":"Overview","text":"<p>Surfing Weights solves the challenge of deploying large AI models to resource-constrained environments by streaming model weights on-demand instead of requiring the entire model to be downloaded upfront.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udeab Zero Local Storage: Stream model weights as needed instead of downloading entire models</li> <li>\ud83d\udce6 Smart Caching: LRU cache for frequently used layers with configurable cache size</li> <li>\ud83d\udcf1 Edge Optimized: Designed for resource-constrained devices (IoT, mobile, embedded)</li> <li>\ud83e\udd17 HuggingFace Compatible: Works with existing transformer models from HuggingFace Hub</li> <li>\u26a1 Async Architecture: Non-blocking inference with async/await support</li> <li>\ud83d\ude80 Production Ready: Monitoring, compression, and distributed caching support</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from streaming_weights import WeightServer\nimport asyncio\n\nasync def start_server():\n    server = WeightServer(\"./chunks/bert-tiny\", port=8765)\n    await server.start_server()\n\nasyncio.run(start_server())\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation Guide - Install Surfing Weights</li> <li>Quick Start - Start streaming weights in minutes</li> <li>Core Concepts - Learn the fundamental concepts</li> </ul>"},{"location":"#why-surfing-weights","title":"Why Surfing Weights?","text":"<p>Traditional approaches to deploying AI models require downloading and storing the entire model locally. This becomes impractical for:</p> <ul> <li>Edge devices with limited storage</li> <li>Mobile applications where model size impacts app size</li> <li>IoT devices with constrained resources</li> <li>Environments requiring multiple model variants</li> </ul> <p>Surfing Weights enables these scenarios by:</p> <ol> <li>Streaming only the required weights on-demand</li> <li>Intelligently caching frequently used layers</li> <li>Minimizing memory usage and network bandwidth</li> <li>Supporting distributed deployment scenarios</li> </ol>"},{"location":"#next-steps","title":"Next Steps","text":"<ol> <li>Follow the Installation Guide to set up Surfing Weights</li> <li>Try the Quick Start Tutorial</li> <li>Explore Example Use Cases</li> <li>Read the API Documentation</li> </ol>"},{"location":"getting-started/concepts/","title":"Core Concepts","text":""},{"location":"getting-started/concepts/#overview","title":"Overview","text":"<p>Surfing Weights is built around several key concepts that enable efficient streaming of model weights. Understanding these concepts will help you make the most of the library.</p>"},{"location":"getting-started/concepts/#weight-chunking","title":"Weight Chunking","text":""},{"location":"getting-started/concepts/#what-is-chunking","title":"What is Chunking?","text":"<p>Chunking is the process of breaking down a large model into smaller, manageable pieces that can be:</p> <ul> <li>Stored efficiently</li> <li>Transmitted quickly</li> <li>Loaded on demand</li> </ul>"},{"location":"getting-started/concepts/#how-chunking-works","title":"How Chunking Works","text":"<ol> <li>Model Analysis</li> <li>The model's architecture is analyzed</li> <li> <p>Weights are grouped by layers</p> </li> <li> <p>Chunk Creation</p> </li> <li>Each layer's weights are saved separately</li> <li>Metadata about chunks is stored</li> <li>Configuration is preserved</li> </ol>"},{"location":"getting-started/concepts/#weight-streaming","title":"Weight Streaming","text":""},{"location":"getting-started/concepts/#the-streaming-process","title":"The Streaming Process","text":"<ol> <li>Initial Setup</li> <li>Client connects to weight server</li> <li>Model architecture is initialized</li> <li> <p>Only metadata is loaded initially</p> </li> <li> <p>On-Demand Loading</p> </li> <li>Weights are requested as needed</li> <li>Server streams requested chunks</li> <li> <p>Client processes received weights</p> </li> <li> <p>Smart Caching</p> </li> <li>Frequently used weights are cached</li> <li>LRU policy manages cache size</li> <li>Cold weights are released</li> </ol>"},{"location":"getting-started/concepts/#storage-backends","title":"Storage Backends","text":"<p>Surfing Weights supports multiple storage backends:</p> <ol> <li>Local Filesystem</li> <li>Direct access to local files</li> <li> <p>Fastest for local deployment</p> </li> <li> <p>Amazon S3</p> </li> <li>Cloud-based storage</li> <li>Scalable and reliable</li> <li> <p>Good for distributed setups</p> </li> <li> <p>Custom Backends</p> </li> <li>Extensible interface</li> <li>Support for other storage systems</li> </ol>"},{"location":"getting-started/concepts/#caching-system","title":"Caching System","text":""},{"location":"getting-started/concepts/#cache-levels","title":"Cache Levels","text":"<ol> <li>Server-Side Cache</li> <li>Reduces storage backend access</li> <li>Shared across clients</li> <li> <p>Configurable size</p> </li> <li> <p>Client-Side Cache</p> </li> <li>Reduces network requests</li> <li>Per-client caching</li> <li>Memory-efficient</li> </ol>"},{"location":"getting-started/concepts/#cache-management","title":"Cache Management","text":"<ul> <li>LRU (Least Recently Used) policy</li> <li>Configurable cache sizes</li> <li>Automatic memory management</li> </ul>"},{"location":"getting-started/concepts/#model-support","title":"Model Support","text":"<p>Surfing Weights supports various transformer architectures:</p> <ul> <li>BERT</li> <li>GPT</li> <li>T5</li> <li>LLaMA</li> <li>Custom models</li> </ul> <p>Each model type has specific: - Chunking strategies - Loading patterns - Optimization techniques</p>"},{"location":"getting-started/concepts/#monitoring","title":"Monitoring","text":"<p>Built-in monitoring provides:</p> <ol> <li>Performance Metrics</li> <li>Request latency</li> <li>Cache hit rates</li> <li> <p>Memory usage</p> </li> <li> <p>Health Checks</p> </li> <li>Server status</li> <li>Backend connectivity</li> <li>Resource utilization</li> </ol>"},{"location":"getting-started/concepts/#next-steps","title":"Next Steps","text":"<ul> <li>See Configuration Guide</li> <li>Learn about Storage Backends</li> <li>Explore API Reference</li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>pip package manager</li> </ul>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>Install Surfing Weights using pip:</p> <pre><code>pip install streaming-weights\n</code></pre> <p>This installs the core package with basic functionality.</p>"},{"location":"getting-started/installation/#installation-with-extra-features","title":"Installation with Extra Features","text":""},{"location":"getting-started/installation/#server-components","title":"Server Components","text":"<p>For running a weight streaming server:</p> <pre><code>pip install streaming-weights[server]\n</code></pre> <p>This includes additional dependencies for: - FastAPI server - Monitoring capabilities - Server-side caching</p>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing to Surfing Weights or running tests:</p> <pre><code>pip install streaming-weights[dev]\n</code></pre> <p>This includes: - Testing frameworks - Development tools - Documentation dependencies</p>"},{"location":"getting-started/installation/#full-installation","title":"Full Installation","text":"<p>To install all optional dependencies:</p> <pre><code>pip install streaming-weights[server,dev]\n</code></pre>"},{"location":"getting-started/installation/#cloud-storage-support","title":"Cloud Storage Support","text":""},{"location":"getting-started/installation/#amazon-s3","title":"Amazon S3","text":"<p>For S3 storage backend support:</p> <pre><code>pip install streaming-weights[s3]\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>Test your installation by running:</p> <pre><code>import streaming_weights\nprint(streaming_weights.__version__)\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Follow the Quick Start Guide to begin using Surfing Weights</li> <li>Learn about Core Concepts</li> <li>Explore Configuration Options</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>This guide will help you get up and running with Surfing Weights in minutes.</p>"},{"location":"getting-started/quick-start/#basic-setup","title":"Basic Setup","text":"<ol> <li>First, install Surfing Weights: <pre><code>pip install streaming-weights[server]\n</code></pre></li> </ol>"},{"location":"getting-started/quick-start/#chunking-a-model","title":"Chunking a Model","text":"<p>Before streaming a model, you need to chunk it into smaller pieces:</p> <pre><code>from streaming_weights import ModelChunker\n\n# Initialize the chunker with a HuggingFace model\nchunker = ModelChunker(\"prajjwal1/bert-tiny\", \"./chunks/bert-tiny\")\n\n# Chunk the model\nchunk_info = chunker.chunk_model()\nprint(f\"Model chunked into {len(chunk_info['chunks'])} pieces\")\n</code></pre>"},{"location":"getting-started/quick-start/#starting-the-weight-server","title":"Starting the Weight Server","text":"<p>Create a server to stream your chunked model:</p> <pre><code>from streaming_weights import WeightServer\nimport asyncio\n\nasync def start_server():\n    # Initialize the server with your chunked model\n    server = WeightServer(\n        model_path=\"./chunks/bert-tiny\",\n        port=8765,\n        cache_size=\"2GB\"  # Optional: Set cache size\n    )\n\n    # Start the server\n    await server.start_server()\n\n# Run the server\nif __name__ == \"__main__\":\n    asyncio.run(start_server())\n</code></pre>"},{"location":"getting-started/quick-start/#client-usage","title":"Client Usage","text":"<p>Connect to the weight server and use the model:</p> <pre><code>from streaming_weights import StreamingBertModel\n\n# Initialize the streaming model\nmodel = StreamingBertModel(\n    server_url=\"http://localhost:8765\",\n    model_name=\"prajjwal1/bert-tiny\"\n)\n\n# Use the model for inference\ntext = \"Hello, world!\"\noutputs = model.encode(text)\n</code></pre>"},{"location":"getting-started/quick-start/#configuration-options","title":"Configuration Options","text":"<p>Basic server configuration:</p> <pre><code>server = WeightServer(\n    model_path=\"./chunks/bert-tiny\",\n    port=8765,\n    cache_size=\"2GB\",\n    compression=True,  # Enable weight compression\n    monitoring=True    # Enable Prometheus metrics\n)\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Core Concepts</li> <li>Explore Configuration Options</li> <li>See Example Use Cases</li> <li>Read about Storage Backends</li> </ul>"},{"location":"user-guide/caching/","title":"Caching System","text":"<p>Surfing Weights implements a sophisticated caching system to optimize performance and memory usage. This guide explains how the caching system works and how to configure it for your needs.</p>"},{"location":"user-guide/caching/#overview","title":"Overview","text":"<p>The caching system operates at two levels:</p> <ol> <li>Server-Side Caching</li> <li>Caches raw model weights</li> <li>Shared across all clients</li> <li> <p>Memory-efficient storage</p> </li> <li> <p>Client-Side Caching</p> </li> <li>Caches loaded model components</li> <li>Per-client cache</li> <li>Optimized for inference</li> </ol>"},{"location":"user-guide/caching/#server-side-cache","title":"Server-Side Cache","text":""},{"location":"user-guide/caching/#configuration","title":"Configuration","text":"<pre><code>from streaming_weights import WeightServer\n\nserver = WeightServer(\n    model_path=\"./chunks/bert-tiny\",\n    cache_size_mb=200  # Set cache size in megabytes\n)\n</code></pre> <p>Command line configuration: <pre><code>streaming-weights-server --chunks-dir ./chunks/bert-tiny \\\n    --cache-size 200  # Cache size in MB\n</code></pre></p>"},{"location":"user-guide/caching/#features","title":"Features","text":"<ol> <li>LRU (Least Recently Used) Eviction</li> <li>Automatically removes least used weights</li> <li>Optimizes memory usage</li> <li> <p>Adapts to access patterns</p> </li> <li> <p>Size-Based Management</p> </li> <li>Configurable maximum size</li> <li>Automatic eviction when full</li> <li>Memory usage monitoring</li> </ol>"},{"location":"user-guide/caching/#client-side-cache","title":"Client-Side Cache","text":""},{"location":"user-guide/caching/#configuration_1","title":"Configuration","text":"<pre><code>from streaming_weights import StreamingBertModel\n\nmodel = StreamingBertModel(\n    model_name=\"prajjwal1/bert-tiny\",\n    cache_size=3  # Number of layers to cache\n)\n</code></pre>"},{"location":"user-guide/caching/#features_1","title":"Features","text":"<ol> <li>Component-Level Caching</li> <li>Caches entire model layers</li> <li>Maintains layer state</li> <li> <p>Optimizes inference speed</p> </li> <li> <p>Smart Prefetching    <pre><code># Enable prefetching for better performance\noutputs = await model.forward_async(\n    input_ids=inputs,\n    enable_prefetch=True,\n    prefetch_count=2  # Prefetch next 2 layers\n)\n</code></pre></p> </li> <li> <p>Cache Warmup    <pre><code># Preload specific layers\nawait model.warmup(layer_indices=[0, 1, 2])\n</code></pre></p> </li> </ol>"},{"location":"user-guide/caching/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"user-guide/caching/#cache-statistics","title":"Cache Statistics","text":"<pre><code># Get cache performance metrics\nstats = model.get_inference_stats()\nprint(f\"Cache hit rate: {stats['cache_hit_rate']:.2%}\")\nprint(f\"Average inference time: {stats['avg_inference_time']:.3f}s\")\n\n# Get current cache state\ncache_info = model.get_cache_info()\nprint(f\"Cached components: {cache_info['cached_components']}\")\nprint(f\"Cache memory usage: {cache_info['memory_usage_mb']:.2f} MB\")\n</code></pre>"},{"location":"user-guide/caching/#cache-management","title":"Cache Management","text":"<pre><code># Clear the cache manually\nmodel.clear_cache()\n\n# Update cache size at runtime\nmodel.cache_size = 5  # Increase cache size\n</code></pre>"},{"location":"user-guide/caching/#advanced-features","title":"Advanced Features","text":""},{"location":"user-guide/caching/#cache-optimization","title":"Cache Optimization","text":"<ol> <li> <p>Access Pattern Optimization    <pre><code># Order layers for optimal caching\nawait model.warmup([0, 1, 2])  # Cache first layers\nawait model.prefetch_next_layers(2, prefetch_count=2)  # Prefetch next layers\n</code></pre></p> </li> <li> <p>Memory Management    <pre><code># Monitor and adjust cache size\nif model.get_cache_info()['memory_usage_mb'] &gt; 1000:\n    model.cache_size = model.cache_size - 1\n</code></pre></p> </li> </ol>"},{"location":"user-guide/caching/#distributed-caching","title":"Distributed Caching","text":"<p>When using multiple servers:</p> <pre><code>from streaming_weights import AdvancedWeightServer\n\nserver = AdvancedWeightServer(\n    chunks_dir=\"./chunks\",\n    redis_url=\"redis://localhost:6379\",  # Redis for distributed caching\n    cache_size_mb=1000\n)\n</code></pre>"},{"location":"user-guide/caching/#best-practices","title":"Best Practices","text":"<ol> <li>Cache Size Configuration</li> <li>Set server cache size based on available RAM</li> <li>Adjust client cache size based on model architecture</li> <li> <p>Monitor cache hit rates for optimization</p> </li> <li> <p>Performance Optimization</p> </li> <li>Use warmup for frequently accessed layers</li> <li>Enable prefetching for sequential access</li> <li> <p>Clear cache when switching tasks</p> </li> <li> <p>Memory Management</p> </li> <li>Monitor memory usage with get_cache_info()</li> <li>Adjust cache sizes based on workload</li> <li>Clear cache when memory pressure is high</li> </ol>"},{"location":"user-guide/caching/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/caching/#common-issues","title":"Common Issues","text":"<ol> <li>High Memory Usage</li> <li>Reduce cache size</li> <li>Clear cache more frequently</li> <li> <p>Monitor with get_cache_info()</p> </li> <li> <p>Poor Cache Performance</p> </li> <li>Check cache hit rates</li> <li>Adjust cache size</li> <li>Review access patterns</li> </ol>"},{"location":"user-guide/caching/#cache-monitoring","title":"Cache Monitoring","text":"<pre><code># Monitor cache performance\nwhile running_inference:\n    stats = model.get_inference_stats()\n    if stats['cache_hit_rate'] &lt; 0.5:\n        print(\"Warning: Low cache hit rate\")\n    await asyncio.sleep(60)  # Check every minute\n</code></pre>"},{"location":"user-guide/caching/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Error Handling</li> <li>Explore Model Support</li> <li>See Example Use Cases</li> </ul>"},{"location":"user-guide/configuration/","title":"Configuration Guide","text":"<p>This guide covers all configuration options available in Surfing Weights, from basic settings to advanced features.</p>"},{"location":"user-guide/configuration/#server-configuration","title":"Server Configuration","text":""},{"location":"user-guide/configuration/#basic-server-settings","title":"Basic Server Settings","text":"<pre><code>from streaming_weights import WeightServer\n\nserver = WeightServer(\n    model_path=\"./chunks/bert-tiny\",  # Path to chunked model\n    host=\"localhost\",                  # Server hostname\n    port=8765,                        # Server port\n    use_ssl=False                     # Enable/disable SSL\n)\n</code></pre>"},{"location":"user-guide/configuration/#cache-configuration","title":"Cache Configuration","text":"<pre><code>server = WeightServer(\n    model_path=\"./chunks/bert-tiny\",\n    cache_size=\"2GB\",              # Server-side cache size\n    enable_compression=True        # Enable weight compression\n)\n</code></pre>"},{"location":"user-guide/configuration/#command-line-options","title":"Command Line Options","text":"<p>When using the CLI:</p> <pre><code># Basic usage\nstreaming-weights-server --chunks-dir ./chunks/bert-tiny --port 8765\n\n# With cache and logging options\nstreaming-weights-server --chunks-dir ./chunks/bert-tiny --port 8765 \\\n    --cache-size 200 --verbose\n</code></pre>"},{"location":"user-guide/configuration/#client-configuration","title":"Client Configuration","text":""},{"location":"user-guide/configuration/#basic-client-settings","title":"Basic Client Settings","text":"<pre><code>from streaming_weights import StreamingBertModel\n\nmodel = StreamingBertModel(\n    model_name=\"prajjwal1/bert-tiny\",\n    server_host=\"localhost\",\n    server_port=8765,\n    use_ssl=False\n)\n</code></pre>"},{"location":"user-guide/configuration/#performance-settings","title":"Performance Settings","text":"<pre><code>model = StreamingBertModel(\n    model_name=\"prajjwal1/bert-tiny\",\n    cache_size=3,           # Number of layers to cache\n    prefetch_layers=True,   # Enable layer prefetching\n    prefetch_count=1,       # Number of layers to prefetch\n    timeout_seconds=30      # Request timeout\n)\n</code></pre>"},{"location":"user-guide/configuration/#storage-backend-configuration","title":"Storage Backend Configuration","text":""},{"location":"user-guide/configuration/#local-filesystem","title":"Local Filesystem","text":"<pre><code>from streaming_weights import WeightServer, FilesystemBackend\n\nstorage = FilesystemBackend(base_path=\"./chunks/bert-tiny\")\nserver = WeightServer(storage_backend=storage)\n</code></pre>"},{"location":"user-guide/configuration/#amazon-s3","title":"Amazon S3","text":"<pre><code>from streaming_weights import WeightServer, S3Backend\n\n# Option 1: Direct initialization\nserver = WeightServer(\n    s3_bucket=\"model-weights\",\n    s3_prefix=\"models/bert-tiny\",\n    s3_region=\"us-east-1\"\n)\n\n# Option 2: Custom backend configuration\nstorage = S3Backend(\n    bucket_name=\"model-weights\",\n    prefix=\"models/bert-tiny\",\n    region_name=\"us-east-1\",\n    aws_access_key_id=\"YOUR_ACCESS_KEY\",      # Optional\n    aws_secret_access_key=\"YOUR_SECRET_KEY\",  # Optional\n    aws_session_token=\"YOUR_SESSION_TOKEN\",   # Optional\n    profile_name=\"default\",                   # Optional\n    endpoint_url=\"https://custom-endpoint\"    # Optional\n)\nserver = WeightServer(storage_backend=storage)\n</code></pre>"},{"location":"user-guide/configuration/#s3-command-line-options","title":"S3 Command Line Options","text":"<pre><code># Basic S3 usage\nstreaming-weights-server --s3 --s3-bucket model-weights \\\n    --s3-prefix models/bert-tiny --port 8765\n\n# With AWS credentials\nstreaming-weights-server --s3 --s3-bucket model-weights \\\n    --s3-access-key YOUR_ACCESS_KEY \\\n    --s3-secret-key YOUR_SECRET_KEY \\\n    --s3-region us-east-1\n</code></pre>"},{"location":"user-guide/configuration/#advanced-features","title":"Advanced Features","text":""},{"location":"user-guide/configuration/#monitoring-configuration","title":"Monitoring Configuration","text":"<p>Enable Prometheus metrics:</p> <pre><code>server = WeightServer(\n    model_path=\"./chunks/bert-tiny\",\n    enable_monitoring=True,\n    metrics_port=9090\n)\n</code></pre>"},{"location":"user-guide/configuration/#ssl-configuration","title":"SSL Configuration","text":"<p>Enable secure WebSocket connections:</p> <pre><code>server = WeightServer(\n    model_path=\"./chunks/bert-tiny\",\n    use_ssl=True,\n    ssl_cert_path=\"path/to/cert.pem\",\n    ssl_key_path=\"path/to/key.pem\"\n)\n</code></pre>"},{"location":"user-guide/configuration/#environment-variables","title":"Environment Variables","text":"<p>Surfing Weights also supports configuration via environment variables:</p> <ul> <li><code>SURFING_SERVER_HOST</code> - Server hostname</li> <li><code>SURFING_SERVER_PORT</code> - Server port</li> <li><code>SURFING_CACHE_SIZE</code> - Cache size in MB</li> <li><code>AWS_ACCESS_KEY_ID</code> - AWS access key</li> <li><code>AWS_SECRET_ACCESS_KEY</code> - AWS secret key</li> <li><code>AWS_SESSION_TOKEN</code> - AWS session token</li> <li><code>AWS_PROFILE</code> - AWS profile name</li> <li><code>AWS_REGION</code> - AWS region</li> </ul>"},{"location":"user-guide/configuration/#configuration-reference","title":"Configuration Reference","text":""},{"location":"user-guide/configuration/#server-settings","title":"Server Settings","text":"Setting Type Default Description server_host str \"localhost\" Server hostname server_port int 8765 Server port use_ssl bool False Enable SSL cache_size int/str \"100MB\" Cache size enable_compression bool True Enable compression"},{"location":"user-guide/configuration/#client-settings","title":"Client Settings","text":"Setting Type Default Description cache_size int 3 Layers to cache prefetch_layers bool True Enable prefetching prefetch_count int 1 Layers to prefetch timeout_seconds int 30 Request timeout"},{"location":"user-guide/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Storage Backends</li> <li>Explore Caching System</li> <li>Read about Error Handling</li> </ul>"},{"location":"user-guide/error-handling/","title":"Error Handling","text":"<p>Surfing Weights implements comprehensive error handling to ensure reliability in production environments. This guide explains how errors are handled at different levels and how to implement proper error handling in your applications.</p>"},{"location":"user-guide/error-handling/#common-error-types","title":"Common Error Types","text":""},{"location":"user-guide/error-handling/#connection-errors","title":"Connection Errors","text":"<ol> <li> <p>WebSocket Connection Errors <pre><code>try:\n    model = StreamingBertModel(websocket_uri=\"ws://localhost:8765\")\n    outputs = await model.forward_async(inputs)\nexcept websockets.exceptions.ConnectionClosed:\n    print(\"Connection to weight server lost\")\nexcept websockets.exceptions.WebSocketException:\n    print(\"WebSocket error occurred\")\nexcept asyncio.TimeoutError:\n    print(\"Connection timed out\")\n</code></pre></p> </li> <li> <p>Storage Backend Errors <pre><code>try:\n    storage = S3Backend(bucket_name=\"model-weights\")\n    await storage.load(\"layer_0.pt\")\nexcept FileNotFoundError:\n    print(\"Model chunk not found\")\nexcept IOError as e:\n    print(f\"Storage error: {e}\")\n</code></pre></p> </li> </ol>"},{"location":"user-guide/error-handling/#automatic-retry-logic","title":"Automatic Retry Logic","text":""},{"location":"user-guide/error-handling/#model-component-loading","title":"Model Component Loading","text":"<p>The base model implements automatic retries for component loading:</p> <pre><code># Retry configuration is built into StreamingBaseModel\nmodel = StreamingBertModel(\n    model_name=\"bert-base\",\n    websocket_uri=\"ws://localhost:8765\",\n    # Default retry settings:\n    # - max_retries = 3\n    # - initial_retry_delay = 1.0 seconds\n    # - exponential_backoff = True\n)\n</code></pre> <p>Internal retry logic: <pre><code># This is handled automatically by the library\nmax_retries = 3\nretry_delay = 1.0\n\nfor attempt in range(max_retries):\n    try:\n        # Attempt to fetch weights\n        weights = await fetch_weights()\n        return weights\n    except ConnectionError:\n        if attempt &lt; max_retries - 1:\n            # Exponential backoff\n            await asyncio.sleep(retry_delay * (2**attempt))\n        else:\n            raise\n</code></pre></p>"},{"location":"user-guide/error-handling/#error-recovery","title":"Error Recovery","text":""},{"location":"user-guide/error-handling/#server-side-recovery","title":"Server-Side Recovery","text":"<ol> <li> <p>Cache Management Errors <pre><code># Server automatically handles cache overflow\nserver = WeightServer(\n    model_path=\"./chunks/bert-tiny\",\n    cache_size_mb=100  # If exceeded, LRU items are evicted\n)\n</code></pre></p> </li> <li> <p>Storage Backend Failover <pre><code>from streaming_weights import WeightServer, S3Backend, FilesystemBackend\n\n# Primary storage (S3)\ns3_storage = S3Backend(bucket_name=\"model-weights\")\n\n# Backup storage (local filesystem)\nbackup_storage = FilesystemBackend(\"./backup_chunks\")\n\ntry:\n    await s3_storage.load(\"layer_0.pt\")\nexcept Exception:\n    # Fallback to backup storage\n    await backup_storage.load(\"layer_0.pt\")\n</code></pre></p> </li> </ol>"},{"location":"user-guide/error-handling/#client-side-recovery","title":"Client-Side Recovery","text":"<ol> <li> <p>Cache Cleanup <pre><code># Automatic cache cleanup on errors\nasync with StreamingBertModel() as model:\n    try:\n        outputs = await model.forward_async(inputs)\n    except Exception:\n        # Cache is automatically cleared in __aexit__\n        pass\n</code></pre></p> </li> <li> <p>Component Reloading <pre><code>async def inference_with_recovery(model, inputs, max_attempts=3):\n    for attempt in range(max_attempts):\n        try:\n            return await model.forward_async(inputs)\n        except Exception:\n            model.clear_cache()  # Clear potentially corrupted cache\n            if attempt == max_attempts - 1:\n                raise\n            await asyncio.sleep(1)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/error-handling/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/error-handling/#1-use-async-context-managers","title":"1. Use Async Context Managers","text":"<pre><code>async with StreamingBertModel() as model:\n    # Resources are properly cleaned up even if errors occur\n    outputs = await model.forward_async(inputs)\n</code></pre>"},{"location":"user-guide/error-handling/#2-implement-health-checks","title":"2. Implement Health Checks","text":"<pre><code>async def check_server_health(uri=\"ws://localhost:8765\"):\n    try:\n        async with websockets.connect(uri, timeout=5):\n            return True\n    except Exception:\n        return False\n\n# Check before starting inference\nif not await check_server_health():\n    raise RuntimeError(\"Weight server is not healthy\")\n</code></pre>"},{"location":"user-guide/error-handling/#3-monitor-error-rates","title":"3. Monitor Error Rates","text":"<pre><code>from streaming_weights import StreamingMonitor\n\nmonitor = StreamingMonitor()\nmodel = StreamingBertModel(monitor=monitor)\n\n# After running inference\nstats = monitor.get_stats()\nerror_rate = stats[\"errors\"] / stats[\"total_requests\"]\nif error_rate &gt; 0.1:  # 10% error rate threshold\n    alert_admin(\"High error rate detected\")\n</code></pre>"},{"location":"user-guide/error-handling/#4-handle-specific-error-types","title":"4. Handle Specific Error Types","text":"<pre><code>from streaming_weights.exceptions import (\n    WeightServerError,\n    StorageError,\n    CacheError,\n    ModelError\n)\n\ntry:\n    model = StreamingBertModel()\n    outputs = await model.forward_async(inputs)\nexcept WeightServerError as e:\n    # Handle server-specific errors\n    logger.error(f\"Weight server error: {e}\")\nexcept StorageError as e:\n    # Handle storage backend errors\n    logger.error(f\"Storage error: {e}\")\nexcept CacheError as e:\n    # Handle cache-related errors\n    logger.error(f\"Cache error: {e}\")\n    model.clear_cache()\nexcept ModelError as e:\n    # Handle model-specific errors\n    logger.error(f\"Model error: {e}\")\n</code></pre>"},{"location":"user-guide/error-handling/#security-considerations","title":"Security Considerations","text":"<ol> <li> <p>Validate Model Chunks <pre><code>from streaming_weights.utils import calculate_chunk_hash\n\n# Verify chunk integrity\nchunk_hash = calculate_chunk_hash(\"./chunks/bert-tiny/layer_0.pt\")\nif chunk_hash != expected_hash:\n    raise SecurityError(\"Chunk integrity check failed\")\n</code></pre></p> </li> <li> <p>Handle Timeouts <pre><code># Configure timeouts for security\nmodel = StreamingBertModel(\n    websocket_uri=\"ws://localhost:8765\",\n    timeout_seconds=30  # Prevent hanging on malicious servers\n)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/error-handling/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Model Support</li> <li>Explore Example Use Cases</li> <li>Review Configuration Options</li> </ul>"},{"location":"user-guide/model-support/","title":"Model Support","text":"<p>Surfing Weights supports various transformer model architectures through its streaming model implementations. This guide covers the supported model types, how to use them, and how to extend support for new architectures.</p>"},{"location":"user-guide/model-support/#supported-model-types","title":"Supported Model Types","text":"<p>Currently, Surfing Weights provides native support for the following model architectures:</p> <ul> <li>BERT Models: Through <code>StreamingBertModel</code></li> <li>LLaMA Models: Through <code>StreamingLlamaModel</code></li> <li>T5 Models: Through <code>StreamingT5Model</code></li> <li>GPT Models: Through <code>StreamingGPTModel</code> (optional dependency)</li> </ul>"},{"location":"user-guide/model-support/#model-architecture-overview","title":"Model Architecture Overview","text":"<p>Each model implementation follows a common architecture pattern:</p> <ol> <li>Lightweight Local Components: </li> <li>Embeddings and final layers are loaded locally</li> <li>These components are typically small and frequently accessed</li> <li> <p>Examples: token embeddings, normalization layers</p> </li> <li> <p>Streamed Components:</p> </li> <li>Transformer layers are loaded on-demand</li> <li>Cached using LRU (Least Recently Used) strategy</li> <li> <p>Automatically evicted when cache is full</p> </li> <li> <p>Smart Caching:</p> </li> <li>Configurable cache size for layers</li> <li>Automatic prefetching of next layers</li> <li>Cache hit/miss statistics tracking</li> </ol>"},{"location":"user-guide/model-support/#using-models","title":"Using Models","text":"<p>Here's an example of using a LLaMA model with streaming weights:</p> <pre><code>from streaming_weights import StreamingLlamaModel, WeightServer\nfrom transformers import LlamaTokenizer\n\n# Initialize the model\nmodel = StreamingLlamaModel(\n    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    websocket_uri=\"ws://localhost:8765\",\n    cache_size=5  # Cache size for layers\n)\n\n# Warm up the model (preload first few layers)\nawait model.warmup([0, 1, 2])\n\n# Use the model\noutputs = await model.forward_async(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    enable_prefetch=True  # Enable automatic prefetching\n)\n</code></pre>"},{"location":"user-guide/model-support/#model-configuration","title":"Model Configuration","text":"<p>All streaming models support the following configuration options:</p> <ul> <li><code>model_name</code>: HuggingFace model identifier or local path</li> <li><code>websocket_uri</code>: WebSocket URI for the weight server</li> <li><code>cache_size</code>: Number of layers to keep in memory</li> </ul>"},{"location":"user-guide/model-support/#advanced-features","title":"Advanced Features","text":""},{"location":"user-guide/model-support/#1-layer-prefetching","title":"1. Layer Prefetching","text":"<p>Models support automatic prefetching of upcoming layers:</p> <pre><code># Enable prefetching in forward pass\noutputs = await model.forward_async(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    enable_prefetch=True\n)\n\n# Manual prefetch of specific layers\nawait model.prefetch_next_layers(current_layer=2, prefetch_count=2)\n</code></pre>"},{"location":"user-guide/model-support/#2-cache-management","title":"2. Cache Management","text":"<p>Monitor and control the layer cache:</p> <pre><code># Get cache statistics\ncache_info = model.get_cache_info()\nprint(f\"Cache hits: {cache_info['hits']}\")\nprint(f\"Cache misses: {cache_info['misses']}\")\nprint(f\"Hit rate: {cache_info['hit_rate']:.2%}\")\n</code></pre>"},{"location":"user-guide/model-support/#3-model-chunking","title":"3. Model Chunking","text":"<p>Before using a model, it needs to be chunked for streaming:</p> <pre><code>from streaming_weights import ModelChunker\n\n# Chunk model to local filesystem\nchunker = ModelChunker(\n    model_name=\"model-name\",\n    output_dir=\"./model_chunks\"\n)\nchunk_info = await chunker.chunk_model()\n\n# Or chunk to S3\nchunker = ModelChunker(\n    model_name=\"model-name\",\n    storage_backend=s3_backend\n)\nchunk_info = await chunker.chunk_model()\n</code></pre>"},{"location":"user-guide/model-support/#storage-backends","title":"Storage Backends","text":"<p>Models can be chunked and stored using different backends:</p> <ol> <li>Filesystem Backend (default):</li> <li>Local storage of model chunks</li> <li> <p>Fastest access for local deployment</p> </li> <li> <p>S3 Backend:</p> </li> <li>Cloud storage of model chunks</li> <li>Good for distributed deployments</li> <li>Automatic compression support</li> </ol>"},{"location":"user-guide/model-support/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Cache Size:</li> <li>Larger cache = better performance but more memory</li> <li> <p>Recommended: 3-5 layers for most use cases</p> </li> <li> <p>Prefetching:</p> </li> <li>Enable for sequential processing</li> <li> <p>Disable for random access patterns</p> </li> <li> <p>Initial Warmup:</p> </li> <li>Preload frequently used layers</li> <li>Reduces initial latency</li> </ol>"},{"location":"user-guide/model-support/#examples","title":"Examples","text":"<p>Check out complete examples in the <code>examples/</code> directory:</p> <ul> <li><code>llama_example.py</code>: Basic LLaMA model usage</li> <li><code>llama_s3_example.py</code>: Using LLaMA with S3 storage</li> <li><code>inference_example.py</code>: Optimized inference setup</li> </ul>"},{"location":"user-guide/model-support/#error-handling","title":"Error Handling","text":"<p>All model operations include proper error handling:</p> <ul> <li>Network errors: Automatic retries</li> <li>Missing layers: Fallback to uninitialized layers</li> <li>Cache errors: Automatic recovery</li> </ul>"},{"location":"user-guide/model-support/#extending-model-support","title":"Extending Model Support","text":"<p>To add support for a new model architecture:</p> <ol> <li>Create a new class inheriting from <code>StreamingBaseModel</code></li> <li>Implement the required methods:</li> <li><code>__init__</code>: Initialize model config and local components</li> <li><code>_load_layer</code>: Layer loading logic</li> <li><code>forward_async</code>: Model-specific forward pass</li> <li>Add chunking support in <code>ModelChunker</code></li> </ol>"},{"location":"user-guide/storage-backends/","title":"Storage Backends","text":"<p>Surfing Weights provides a flexible storage backend system for storing and retrieving model chunks. The storage backend interface is designed to be extensible, allowing you to implement custom backends for different storage solutions.</p>"},{"location":"user-guide/storage-backends/#available-backends","title":"Available Backends","text":""},{"location":"user-guide/storage-backends/#filesystem-backend","title":"FileSystem Backend","text":"<p>The filesystem backend stores model chunks as files in a local directory. This is the simplest backend and is ideal for local development and testing.</p> <pre><code>from streaming_weights import WeightServer, FilesystemBackend\n\n# Initialize the backend\nstorage = FilesystemBackend(base_dir=\"./chunks/bert-tiny\")\n\n# Use with weight server\nserver = WeightServer(storage_backend=storage)\n</code></pre> <p>Key features: - Direct file system access - No additional dependencies - Fastest for local development - Automatic directory creation - Asynchronous I/O for large files</p>"},{"location":"user-guide/storage-backends/#s3-backend","title":"S3 Backend","text":"<p>The S3 backend stores model chunks in Amazon S3 or S3-compatible storage. This is ideal for production deployments and distributed systems.</p>"},{"location":"user-guide/storage-backends/#basic-usage","title":"Basic Usage","text":"<pre><code>from streaming_weights import WeightServer, S3Backend\n\n# Initialize with minimal configuration\nstorage = S3Backend(\n    bucket_name=\"model-weights\",\n    prefix=\"models/bert-tiny\",  # Optional: folder within bucket\n    region_name=\"us-east-1\"    # Optional: AWS region\n)\n\n# Use with weight server\nserver = WeightServer(storage_backend=storage)\n</code></pre>"},{"location":"user-guide/storage-backends/#authentication-options","title":"Authentication Options","text":"<p>The S3 backend supports multiple authentication methods:</p> <ol> <li> <p>Direct credentials: <pre><code>storage = S3Backend(\n    bucket_name=\"model-weights\",\n    aws_access_key_id=\"YOUR_ACCESS_KEY\",\n    aws_secret_access_key=\"YOUR_SECRET_KEY\",\n    aws_session_token=\"YOUR_SESSION_TOKEN\"  # Optional: for temporary credentials\n)\n</code></pre></p> </li> <li> <p>AWS profile: <pre><code>storage = S3Backend(\n    bucket_name=\"model-weights\",\n    profile_name=\"default\"  # Use specific AWS credentials profile\n)\n</code></pre></p> </li> <li> <p>Environment variables: <pre><code>export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY\nexport AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY\nexport AWS_SESSION_TOKEN=YOUR_SESSION_TOKEN  # Optional\nexport AWS_REGION=us-east-1                 # Optional\n</code></pre></p> </li> <li> <p>Instance profile or ECS task role (when running on AWS)</p> </li> </ol>"},{"location":"user-guide/storage-backends/#s3-compatible-storage","title":"S3-Compatible Storage","text":"<p>Support for S3-compatible storage services (MinIO, DigitalOcean Spaces, etc.):</p> <pre><code>storage = S3Backend(\n    bucket_name=\"model-weights\",\n    endpoint_url=\"https://custom-endpoint\",\n    aws_access_key_id=\"YOUR_ACCESS_KEY\",\n    aws_secret_access_key=\"YOUR_SECRET_KEY\"\n)\n</code></pre>"},{"location":"user-guide/storage-backends/#common-operations","title":"Common Operations","text":"<p>All storage backends implement the following interface:</p>"},{"location":"user-guide/storage-backends/#loading-data","title":"Loading Data","text":"<pre><code># Check if a chunk exists\nexists = await storage.exists(\"layer_0.pt\")\n\n# Load a chunk\ndata = await storage.load(\"layer_0.pt\")\n</code></pre>"},{"location":"user-guide/storage-backends/#saving-data","title":"Saving Data","text":"<pre><code># Save bytes\nawait storage.save(\"layer_0.pt\", chunk_data)\n\n# Save from a file-like object\nwith open(\"layer_0.pt\", \"rb\") as f:\n    await storage.save(\"layer_0.pt\", f)\n</code></pre>"},{"location":"user-guide/storage-backends/#listing-contents","title":"Listing Contents","text":"<pre><code># List all chunks\nchunks = await storage.list()\n\n# List chunks with prefix\nembeddings = await storage.list(\"embeddings_\")\n</code></pre>"},{"location":"user-guide/storage-backends/#creating-custom-backends","title":"Creating Custom Backends","text":"<p>You can create custom storage backends by implementing the <code>StorageBackend</code> abstract base class:</p> <pre><code>from streaming_weights.storage import StorageBackend\nfrom typing import Union, BinaryIO, List\n\nclass CustomBackend(StorageBackend):\n    async def load(self, key: str) -&gt; bytes:\n        \"\"\"Load data from storage\"\"\"\n        pass\n\n    async def save(self, key: str, data: Union[bytes, BinaryIO]) -&gt; None:\n        \"\"\"Save data to storage\"\"\"\n        pass\n\n    async def exists(self, key: str) -&gt; bool:\n        \"\"\"Check if data exists\"\"\"\n        pass\n\n    async def list(self, prefix: str = \"\") -&gt; List[str]:\n        \"\"\"List all keys with given prefix\"\"\"\n        pass\n</code></pre>"},{"location":"user-guide/storage-backends/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling</li> <li>Handle storage-specific errors gracefully</li> <li>Provide meaningful error messages</li> <li> <p>Implement proper retries for transient failures</p> </li> <li> <p>Performance</p> </li> <li>Use appropriate chunk sizes</li> <li>Enable compression when beneficial</li> <li> <p>Implement caching for frequently accessed chunks</p> </li> <li> <p>Security</p> </li> <li>Use secure credentials management</li> <li>Implement proper access controls</li> <li>Enable encryption at rest when needed</li> </ol>"},{"location":"user-guide/storage-backends/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about the Caching System</li> <li>Explore Error Handling</li> <li>See Example Use Cases</li> </ul>"}]}